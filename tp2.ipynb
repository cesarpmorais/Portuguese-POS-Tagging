{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP TP 2 - POS Tagging em Português\n",
    "César de Paula Morais - 2021031521"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nesse trabalho, implementarei um modelo de NLP para POS-Tagging em português, a partir da base de treinamento Mac-morpho.\n",
    "\n",
    "Como vimos em aula, há diversas maneiras de fazer o POS Tagging. A que mais me interessou, por curiosidade do resultado e da implementação em si, é o **Encoder-Only Transformer** - pode ser que a acurácia não seja tão alta nesse método, mas é uma boa oportunidade de explorar o potencial dos **context-aware embeddings**! <br />\n",
    "Além disso, algo que me motivou a usar esse método foram as [aulas do canal StatQuest sobre o assunto](https://www.youtube.com/watch?v=GDN649X_acE&ab_channel=StatQuestwithJoshStarmer), em que é mencionado o poderio muitas vezes subestimado desse método (em favor dos Decoder-Only Transformers, famosos pelo GPT).\n",
    "\n",
    "Na prática, usarei o [BERTimbau](https://huggingface.co/neuralmind/bert-base-portuguese-cased), um modelo BERT pré-treinado em português, para gerar os context-aware embeddings. Em seguida, com o Tensorflow, criei uma camada densa, de dimensão *tamanho do embedding BERTimbau -> número total de classes*. A entrada dessa camada será uma frase (formada pelos embeddings), e a saída será uma softmax com a predição da classe. <br />\n",
    "Ao final, iremos verificar os resultados para as diferentes classes e apontar forças/fraquezas do método, tanto como possibilidade de melhoria."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funções Auxiliares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primeiramente, definiremos os imports, e definiremos o tokenizer e modelo do BERTimbau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "NUMBER_OF_LABELS = 30\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"neuralmind/bert-base-portuguese-cased\", max_length=512, truncation=True)\n",
    "bert_model = AutoModel.from_pretrained(\"neuralmind/bert-base-portuguese-cased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As funções abaixo são responsáveis por ler as labels do macmorpho (esse arquivo foi escrito à mão, com ajuda da documentação) e carregar esses dados em vetores ordenados de frases e suas labels, já em formato de número (a entrada da camada densa).\n",
    "\n",
    "Um detalhe de implementação é que, na função `load_macmorpho`, já tokenizamos as palavras com o BERTimbau. Isso ocorre pois não necessariamente o modelo irá tokenizar as palavras a partir dos caracteres de espaço - pode ser que uma palavra seja tokenizada por duas ou mais. Caso isso ocorra, estamos marcando a segunda parte dessa palavra como \"inutilizada\". Tal tratamento ocorre para evitar problemas de dimensionalidade durante o treinamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels(labels_path=\"input/macmorpho-labels\"):\n",
    "    lines = []\n",
    "\n",
    "    with open(labels_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        for line in file:\n",
    "            lines.append(line.strip())\n",
    "\n",
    "    return lines\n",
    "\n",
    "def load_macmorpho(file_path, tokenizer, label_list):\n",
    "    sentences = []\n",
    "    labels = []\n",
    "\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        current_sentence = []\n",
    "        current_label = []\n",
    "\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            tokens = line.split(\" \")\n",
    "\n",
    "            for token in tokens:\n",
    "                word, label = token.rsplit(\"_\", 1)\n",
    "                subwords = tokenizer.tokenize(word)  # Subword tokenization\n",
    "                subword_count = len(subwords)\n",
    "\n",
    "                current_sentence.extend(subwords)\n",
    "                current_label.extend([label_list.index(label)] + [-1] * (subword_count - 1))  # -1 marks subwords that should not contribute to loss\n",
    "\n",
    "            sentences.append(current_sentence)\n",
    "            labels.append(current_label)\n",
    "            current_sentence = []\n",
    "            current_label = []\n",
    "\n",
    "    return sentences, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A partir das frases e labels, então, geramos os embeddings, e depois faremos um tratamento para os tokens inválidos do passo anterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_embeddings(sentences, labels):\n",
    "    sentence_embeddings = []\n",
    "    expanded_labels = []\n",
    "\n",
    "    for sentence, label in zip(sentences, labels):\n",
    "        input_ids = tokenizer(sentence, return_tensors='pt', is_split_into_words=True, padding=True, truncation=True)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outs = bert_model(**input_ids)\n",
    "            token_embeddings = outs.last_hidden_state.squeeze(0)[1:-1]  # Remove [CLS] and [SEP]\n",
    "\n",
    "        # Align labels with tokens\n",
    "        word_ids = input_ids.word_ids() \n",
    "        expanded_label = []\n",
    "        for word_id in word_ids:\n",
    "            if word_id is None:\n",
    "                continue\n",
    "            elif expanded_label and word_id == expanded_label[-1]:\n",
    "                expanded_label.append(-1)\n",
    "            else:\n",
    "                expanded_label.append(label[word_id])\n",
    "\n",
    "        # Ensure embeddings and labels match\n",
    "        if len(token_embeddings) != len(expanded_label):\n",
    "            raise ValueError(f\"Mismatch between tokens and labels: {len(token_embeddings)} vs {len(expanded_label)}\")\n",
    "\n",
    "        sentence_embeddings.append(token_embeddings)\n",
    "        expanded_labels.extend(expanded_label)\n",
    "\n",
    "    return torch.cat(sentence_embeddings).numpy(), np.array(expanded_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treinamento\n",
    "Vamos começar gerando os embeddings de treinamento. Essa é a fase com maior custo computacional, definitivamente. Por isso, optei por salvar os embeddings em um arquivo pickle, caso queira treinar novamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Reading training file...\")\n",
    "training_sentence_tokens, training_labels = load_macmorpho(\"input/macmorpho-train.txt\", tokenizer, get_labels())\n",
    "\n",
    "if os.path.exists(\"output/training_embeddings.pkl\"):\n",
    "    print(f\"Loading embeddings...\")\n",
    "    with open(\"output/training_embeddings.pkl\", \"rb\") as f:\n",
    "        data = pickle.load(f)\n",
    "    X_train, y_train = data[\"embeddings\"], data[\"labels\"]\n",
    "else:\n",
    "    print(f\"Computing embeddings...\")\n",
    "    X_train, y_train = get_sentence_embeddings(training_sentence_tokens, training_labels)\n",
    "\n",
    "    with open(\"output/training_embeddings.pkl\", \"wb\") as f:\n",
    "        pickle.dump({\"embeddings\": X_train, \"labels\": y_train}, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora, vamos definir o modelo da camada densa. Novamente, tem as dimensões de entrada do tamanho do embedding, e sua ativação é uma softmax.\n",
    "\n",
    "Os parâmetros (otimizador, learning_rate, epochs, etc) foram decididos de forma empírica. Foi usada uma split de validação de 20%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cesar/.local/lib/python3.10/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "W0000 00:00:1737140308.420241    1464 gpu_device.cc:2344] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training\n",
      "Epoch 1/10\n",
      "\u001b[1m17734/17734\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 724us/step - accuracy: 0.9283 - loss: 0.2808 - val_accuracy: 0.9442 - val_loss: 0.1981\n",
      "Epoch 2/10\n",
      "\u001b[1m17734/17734\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 686us/step - accuracy: 0.9627 - loss: 0.1303 - val_accuracy: 0.9444 - val_loss: 0.1995\n",
      "Epoch 3/10\n",
      "\u001b[1m17734/17734\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 688us/step - accuracy: 0.9642 - loss: 0.1241 - val_accuracy: 0.9465 - val_loss: 0.1959\n",
      "Epoch 4/10\n",
      "\u001b[1m17734/17734\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 688us/step - accuracy: 0.9645 - loss: 0.1214 - val_accuracy: 0.9441 - val_loss: 0.2010\n",
      "Epoch 5/10\n",
      "\u001b[1m17734/17734\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 685us/step - accuracy: 0.9653 - loss: 0.1198 - val_accuracy: 0.9465 - val_loss: 0.1969\n",
      "Epoch 6/10\n",
      "\u001b[1m17734/17734\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 686us/step - accuracy: 0.9654 - loss: 0.1182 - val_accuracy: 0.9455 - val_loss: 0.2011\n",
      "Epoch 7/10\n",
      "\u001b[1m17734/17734\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 689us/step - accuracy: 0.9658 - loss: 0.1163 - val_accuracy: 0.9463 - val_loss: 0.1995\n",
      "Epoch 8/10\n",
      "\u001b[1m17734/17734\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 702us/step - accuracy: 0.9660 - loss: 0.1159 - val_accuracy: 0.9455 - val_loss: 0.2036\n",
      "Epoch 9/10\n",
      "\u001b[1m17734/17734\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 710us/step - accuracy: 0.9664 - loss: 0.1155 - val_accuracy: 0.9459 - val_loss: 0.2029\n",
      "Epoch 10/10\n",
      "\u001b[1m17734/17734\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 695us/step - accuracy: 0.9663 - loss: 0.1152 - val_accuracy: 0.9471 - val_loss: 0.2013\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7f630c30f040>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    Dense(NUMBER_OF_LABELS, input_dim=768, activation='softmax')  # 768 is the size of the BERTimbau embedding\n",
    "])\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=0.001),\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Removes invalid tokes\n",
    "valid_indices = y_train != -1\n",
    "X_train = X_train[valid_indices]\n",
    "y_train = y_train[valid_indices]\n",
    "\n",
    "print(\"Starting training\")\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Teste\n",
    "O mesmo procedimento será feito no arquivo de teste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading test file...\n",
      "Loading embeddings...\n",
      "Testing...\n",
      "\u001b[1m9264/9264\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 518us/step\n"
     ]
    }
   ],
   "source": [
    "print(\"Reading test file...\")\n",
    "testing_sentence_tokens, testing_labels = load_macmorpho(\"input/macmorpho-test.txt\", tokenizer, get_labels())\n",
    "\n",
    "if os.path.exists(\"output/testing_embeddings.pkl\"):\n",
    "    print(f\"Loading embeddings...\")\n",
    "    with open(\"output/testing_embeddings.pkl\", \"rb\") as f:\n",
    "        data = pickle.load(f)\n",
    "    X_test, y_test = data[\"embeddings\"], data[\"labels\"]\n",
    "else:\n",
    "    print(f\"Computing embeddings...\")\n",
    "    X_test, y_test = get_sentence_embeddings(testing_sentence_tokens, testing_labels)\n",
    "\n",
    "    with open(\"output/testing_embeddings.pkl\", \"wb\") as f:\n",
    "        pickle.dump({\"embeddings\": X_test, \"labels\": y_test}, f)\n",
    "\n",
    "print(\"Testing...\")\n",
    "prediction = model.predict(X_test)\n",
    "predicted_classes = np.argmax(prediction, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente, obteremos o resultado com ajuda da função classification report do sklearn - e imprimindo com os nomes das classes do MacMorpho."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ADJ       0.66      0.86      0.75      5256\n",
      "         ADV       0.84      0.68      0.75       227\n",
      "  ADV-KS-REL       0.93      0.99      0.96     12328\n",
      "         ART       0.87      0.97      0.92      4430\n",
      "         CUR       0.88      1.00      0.94     26328\n",
      "          IN       0.29      0.95      0.45     35414\n",
      "          KC       0.85      0.91      0.88      2487\n",
      "          KS       0.32      0.51      0.40        98\n",
      "           N       0.57      0.94      0.71     15535\n",
      "       NPROP       0.66      0.93      0.77      2481\n",
      "         NUM       0.72      0.81      0.76      3541\n",
      "         PCP       0.81      0.85      0.83      1081\n",
      "        PDEN       0.90      0.97      0.93     16369\n",
      "        PREP       0.64      0.94      0.76      3328\n",
      "    PREP+ADV       0.31      0.87      0.46      8335\n",
      "    PREP+ART       0.77      0.97      0.86      2792\n",
      " PREP+PRO-KS       1.00      0.00      0.00    122753\n",
      " PREP+PROADJ       0.44      0.77      0.56       153\n",
      "PREP+PROPESS       0.84      0.84      0.84        58\n",
      " PREP+PROSUB       0.92      0.90      0.91       125\n",
      "      PRO-KS       0.91      0.99      0.95      9972\n",
      "  PRO-KS-REL       0.74      0.81      0.78      1540\n",
      "     PROPESS       0.89      0.96      0.92       303\n",
      "      PROSUB       0.76      0.97      0.85     19084\n",
      "          PU       0.86      0.92      0.89      2143\n",
      "        VAUX       0.52      1.00      0.68       287\n",
      "\n",
      "    accuracy                           0.56    296448\n",
      "   macro avg       0.73      0.86      0.75    296448\n",
      "weighted avg       0.81      0.56      0.45    296448\n",
      "\n"
     ]
    }
   ],
   "source": [
    "classes = get_labels()\n",
    "mapped_y_test = [classes[i-1] for i in y_test]\n",
    "mapped_predicted_classes = [classes[i-1] for i in predicted_classes]\n",
    "\n",
    "print(classification_report(mapped_y_test, mapped_predicted_classes, zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resultados\n",
    "Em geral, tivemos uma acurácia de 56%, um valor que pode ser melhorado para POS Tagging. Mesmo assim, creio que o experimento foi bem-sucedido no aprendizado dos **Encoder-Only Transformers** - a implementação foi desafiadora e o uso do BERTimbau, modelo conhecido no ambiente de NLP, foi importante.\n",
    "\n",
    "Percebe-se um desbalanço de classes - algumas possuem muitas instâncias (como PREP+PRO-KS), enquanto outras ocorrem raramente (PREP+PROPESS). É uma tendência do modelo acertar mais as classes mais dominantes. Ou seja, esse desbalanço pode ter feito o modelo tender a predizer mais uma classe do que outra. \n",
    "\n",
    "A comparação da macro average e weighted average da precisão também indica isso, já que a weighted average (que leva em consideração o número de instâncias) é maior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Análise do Método e Modelo\n",
    "- **Pontos positivos:** o aprendizado desse tipo de transformer foi o principal ponto positivo! Além disso, os resultados foram interessantes, com acurácia beirando os 60%.\n",
    "- **Pontos a melhorar**: a performance em classes menos significativas foi menor, no geral. Há técnicas para mitigação, como, por exemplo, o uso de pesos para classes durante o treinamento, ou fazer *undersampling/oversampling*, também no treinamento.\n",
    "- **Passos extras**: outra forma de, talvez, melhorar a acurácia do modelo é alterar os parâmetros da camada densa e do treinamento, por meio de métodos como GridSearch."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
